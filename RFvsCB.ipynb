{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uncertainty Quantification in Random Forests vs CatBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import precision_score, recall_score, accuracy_score, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from math import log\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prob_matrix(model, x_test, n_estimators, laplace_smoothing, log=False):\n",
    "    porb_matrix = [[[] for j in range(n_estimators)] for i in range(x_test.shape[0])]\n",
    "    for etree in range(n_estimators):\n",
    "        # populate the porb_matrix with the tree_prob\n",
    "        tree_prob = model.estimators_[etree].predict_proba(x_test)\n",
    "        if laplace_smoothing > 0:\n",
    "            leaf_index_array = model.estimators_[etree].apply(x_test)\n",
    "            for data_index, leaf_index in enumerate(leaf_index_array):\n",
    "                leaf_values = model.estimators_[etree].tree_.value[leaf_index]\n",
    "                leaf_samples = np.array(leaf_values).sum()\n",
    "                for i,v in enumerate(leaf_values[0]):\n",
    "                    tmp = (v + laplace_smoothing) / (leaf_samples + (len(leaf_values[0]) * laplace_smoothing))\n",
    "                    tree_prob[data_index][i] = (v + laplace_smoothing) / (leaf_samples + (len(leaf_values[0]) * laplace_smoothing))\n",
    "\n",
    "        for data_index, data_prob in enumerate(tree_prob):\n",
    "            porb_matrix[data_index][etree] = list(data_prob)\n",
    "\n",
    "        if log:\n",
    "            print(f\"----------------------------------------[{etree}]\")\n",
    "            print(f\"class {model.estimators_[etree].predict(x_test)}  prob \\n{tree_prob}\")\n",
    "    return porb_matrix\n",
    "\n",
    "def uncertainty_estimate(probs): # three dimentianl array with d1 as datapoints, (d2) the rows as samples and (d3) the columns as probability for each class\n",
    "    p = np.array(probs)\n",
    "    entropy = -p*np.ma.log10(p)\n",
    "    entropy = entropy.filled(0)\n",
    "    a = np.sum(entropy, axis=1)\n",
    "    a = np.sum(a, axis=1) / entropy.shape[1]\n",
    "    p_m = np.mean(p, axis=1)\n",
    "    total = -np.sum(p_m*np.ma.log10(p_m), axis=1)\n",
    "    total = total.filled(0)\n",
    "    e = total - a\n",
    "    return total, e, a\n",
    "\n",
    "def transpose_stage_predictions(staged_preds):\n",
    "    \n",
    "    model_probas = list(staged_preds)\n",
    "    mbier\n",
    "    for i in range(len(model_probas[0])):\n",
    "        model_probas_transposed.append([el[i] for el in model_probas])\n",
    "    return model_probas_transposed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load SPECT Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('data/SPECT .train', delimiter=',', header=None)\n",
    "test = pd.read_csv('data/SPECT .test', delimiter=',',header=None)\n",
    "data = pd.concat([train,test]).reset_index(drop=True)\n",
    "x = data.drop(0, axis=1)\n",
    "y = data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_ARC_eU_list, rf_ARC_aU_list, rf_ARC_random_list = [],[],[]\n",
    "cb_ARC_eU_list, cb_ARC_aU_list, cb_ARC_random_list = [],[],[]\n",
    "\n",
    "rf_criterion='entropy'\n",
    "rf_max_depth=10\n",
    "rf_n_estimators=50\n",
    "rf_laplace_smoothing = 1\n",
    "\n",
    "cb_n_trees = 25\n",
    "cb_max_depth = 3\n",
    "test_size=0.3\n",
    "\n",
    "\n",
    "for i in range(100):\n",
    "    \n",
    "    # random train_test split\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=test_size,shuffle=True)\n",
    "    x_train, x_test, y_train, y_test = x_train.copy(), x_test.copy(), y_train.copy(), y_test.copy()\n",
    "\n",
    "    \n",
    "    # init models\n",
    "    rf_model = RandomForestClassifier(bootstrap=True,\n",
    "                                        criterion=rf_criterion,\n",
    "                                        max_depth=rf_max_depth,\n",
    "                                        n_estimators=rf_n_estimators,\n",
    "                                        random_state=None,\n",
    "                                        verbose=0,\n",
    "                                        warm_start=False)\n",
    "    \n",
    "    cb_model = CatBoostClassifier(iterations=cb_n_trees,\n",
    "                               learning_rate=0.5,\n",
    "                               depth=cb_max_depth,\n",
    "                              cat_features=x_train.columns-1,\n",
    "                              silent=True)\n",
    "    \n",
    "    \n",
    "    # fit models\n",
    "    rf_model.fit(x_train, y_train)\n",
    "    cb_model.fit(x_train, y_train)\n",
    "\n",
    "    # let models predict test data\n",
    "    rf_preds_class = rf_model.predict(x_test)\n",
    "    cb_preds_class = cb_model.predict(x_test)\n",
    "\n",
    "    # evaluate accuracy\n",
    "    rf_accuracy = accuracy_score(y_test, rf_preds_class)\n",
    "    cb_accuracy = accuracy_score(y_test, cb_preds_class)\n",
    "    \n",
    "    # calculate uncertainties\n",
    "    rf_prob_matrix = get_prob_matrix(rf_model, x_test, rf_n_estimators, rf_laplace_smoothing, log=0)\n",
    "    rf_total_uncertainty, rf_epistemic_uncertainty, rf_aleatoric_uncertainty = uncertainty_estimate(np.array(rf_prob_matrix))\n",
    "    \n",
    "    cb_staged_preds = cb_model.staged_predict(x_test,\n",
    "                   prediction_type='Probability',\n",
    "                   ntree_start=0, \n",
    "                   ntree_end=10)\n",
    "    cb_prob_matrix = transpose_stage_predictions(cb_staged_preds)\n",
    "    cb_total_uncertainty, cb_epistemic_uncertainty, cb_aleatoric_uncertainty = uncertainty_estimate(np.array(cb_prob_matrix))\n",
    "    \n",
    "    # append model uncertainties to samples\n",
    "    rf_eU = x_test.copy()\n",
    "    rf_aU = x_test.copy()\n",
    "    cb_eU = x_test.copy()\n",
    "    cb_aU = x_test.copy()\n",
    "    \n",
    "    rf_eU ['e'] = rf_epistemic_uncertainty\n",
    "    rf_aU ['a'] = rf_aleatoric_uncertainty\n",
    "    \n",
    "    cb_eU ['e'] = cb_epistemic_uncertainty\n",
    "    cb_aU ['a'] = cb_aleatoric_uncertainty\n",
    "                    \n",
    "    \n",
    "    # sort samples by specific uncertainties\n",
    "    rf_aU.sort_values('a', ascending=True, inplace=True)\n",
    "    rf_eU.sort_values('e', ascending=True, inplace=True)\n",
    "    \n",
    "    cb_aU.sort_values('a', ascending=True, inplace=True)\n",
    "    cb_eU.sort_values('e', ascending=False, inplace=True)\n",
    "    \n",
    "    # init lists, which will contain accuracy of each classifier\n",
    "    # for each rejection rate after loop\n",
    "    \n",
    "    rf_ARC_eU = [rf_accuracy]\n",
    "    rf_ARC_aU = [rf_accuracy]\n",
    "    rf_ARC_random = [rf_accuracy]\n",
    "    cb_ARC_eU = [cb_accuracy]\n",
    "    cb_ARC_aU = [cb_accuracy]\n",
    "    cb_ARC_random = [cb_accuracy]\n",
    "    \n",
    "    # init params for ARC\n",
    "    num_rows = x_test.shape[0]\n",
    "    discard_count = 0\n",
    "    step_size = 0.05\n",
    "    \n",
    "    \n",
    "    current_step = 0.05\n",
    "    while current_step < 1:\n",
    "        discard_count = int (num_rows*current_step)\n",
    "        \n",
    "        # select a portion i of samples, for which uc is lowest \n",
    "        rf_eU_samples = rf_eU.iloc[:-discard_count,0:22]\n",
    "        rf_aU_samples = rf_aU.iloc[:-discard_count,0:22]\n",
    "        \n",
    "        cb_eU_samples = cb_eU.iloc[:-discard_count,0:22]\n",
    "        cb_aU_samples = cb_aU.iloc[:-discard_count,0:22]\n",
    "        \n",
    "        # select a portion i of samples randomly\n",
    "        random_samples = x_test.iloc[:,0:22].sample((num_rows-discard_count))\n",
    "        \n",
    "        # let models predict selected samples\n",
    "        rf_preds_class_eU = rf_model.predict(rf_eU_samples)\n",
    "        rf_preds_class_aU = rf_model.predict(rf_aU_samples)\n",
    "        rf_preds_class_random = rf_model.predict(random_samples)\n",
    "        \n",
    "        cb_preds_class_eU = cb_model.predict(cb_eU_samples)\n",
    "        cb_preds_class_aU = cb_model.predict(cb_aU_samples)\n",
    "        cb_preds_class_random = cb_model.predict(random_samples)\n",
    "        \n",
    "        # evaluate models regarding accuracy\n",
    "        rf_accuracy_eu = accuracy_score(y_test[rf_eU_samples.index], rf_preds_class_eU)\n",
    "        rf_accuracy_au = accuracy_score(y_test[rf_aU_samples.index], rf_preds_class_aU)\n",
    "        rf_accuracy_random = accuracy_score(y_test[random_samples.index], rf_preds_class_random)\n",
    "        \n",
    "        cb_accuracy_eu = accuracy_score(y_test[cb_eU_samples.index], cb_preds_class_eU)\n",
    "        cb_accuracy_au = accuracy_score(y_test[cb_aU_samples.index], cb_preds_class_aU)\n",
    "        cb_accuracy_random = accuracy_score(y_test[random_samples.index], cb_preds_class_random)\n",
    "        \n",
    "        # append eval results to list \n",
    "        rf_ARC_eU.append(rf_accuracy_eu)\n",
    "        rf_ARC_aU.append(rf_accuracy_au)\n",
    "        rf_ARC_random.append(rf_accuracy_random)\n",
    "        \n",
    "        cb_ARC_eU.append(cb_accuracy_eu)\n",
    "        cb_ARC_aU.append(cb_accuracy_au)\n",
    "        cb_ARC_random.append(cb_accuracy_random)\n",
    "        \n",
    "        current_step+=step_size\n",
    "    \n",
    "    # append list of eval results for one iteration to list\n",
    "    rf_ARC_eU_list.append(rf_ARC_eU)\n",
    "    rf_ARC_aU_list.append(rf_ARC_aU)\n",
    "    rf_ARC_random_list.append(rf_ARC_random)\n",
    "    \n",
    "    cb_ARC_eU_list.append(cb_ARC_eU)\n",
    "    cb_ARC_aU_list.append(cb_ARC_aU)\n",
    "    cb_ARC_random_list.append(cb_ARC_random)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate mean values over all iterations for  both classifiers\n",
    "\n",
    "rf_ARC_eU_mean, rf_ARC_aU_mean, rf_ARC_random_mean = [],[],[]\n",
    "cb_ARC_eU_mean, cb_ARC_aU_mean, cb_ARC_random_mean = [],[],[]\n",
    "\n",
    "for i in range(len(rf_ARC_eU_list[0])):\n",
    "    rf_ARC_eU_mean.append(np.array([el[i] for el in rf_ARC_eU_list]).mean())\n",
    "    rf_ARC_aU_mean.append(np.array([el[i] for el in rf_ARC_aU_list]).mean())\n",
    "    rf_ARC_random_mean.append(np.array([el[i] for el in rf_ARC_random_list]).mean())\n",
    "    \n",
    "    cb_ARC_eU_mean.append(np.array([el[i] for el in cb_ARC_eU_list]).mean())\n",
    "    cb_ARC_aU_mean.append(np.array([el[i] for el in cb_ARC_aU_list]).mean())\n",
    "    cb_ARC_random_mean.append(np.array([el[i] for el in cb_ARC_random_list]).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tabular Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest\n",
      "Reject   E    A    R \n",
      "0.00:  0.82 0.82 0.82 \n",
      "0.05:  0.82 0.83 0.82 \n",
      "0.10:  0.83 0.84 0.82 \n",
      "0.15:  0.83 0.85 0.82 \n",
      "0.20:  0.84 0.88 0.82 \n",
      "0.25:  0.84 0.90 0.82 \n",
      "0.30:  0.85 0.92 0.82 \n",
      "0.35:  0.85 0.93 0.82 \n",
      "0.40:  0.86 0.94 0.83 \n",
      "0.45:  0.86 0.94 0.82 \n",
      "0.50:  0.87 0.94 0.81 \n",
      "0.55:  0.87 0.95 0.83 \n",
      "0.60:  0.88 0.96 0.82 \n",
      "0.65:  0.89 0.96 0.82 \n",
      "0.70:  0.90 0.97 0.82 \n",
      "0.75:  0.90 0.97 0.82 \n",
      "0.80:  0.91 0.99 0.81 \n",
      "0.85:  0.91 0.99 0.82 \n",
      "0.90:  0.94 1.00 0.83 \n",
      "0.95:  0.99 1.00 0.81 \n",
      "\n",
      "CatBoost\n",
      "Reject   E    A    R \n",
      "0.00:  0.82 0.82 0.82 \n",
      "0.05:  0.83 0.83 0.82 \n",
      "0.10:  0.83 0.84 0.82 \n",
      "0.15:  0.84 0.85 0.82 \n",
      "0.20:  0.85 0.86 0.82 \n",
      "0.25:  0.86 0.88 0.82 \n",
      "0.30:  0.86 0.89 0.82 \n",
      "0.35:  0.86 0.91 0.82 \n",
      "0.40:  0.87 0.92 0.82 \n",
      "0.45:  0.87 0.93 0.82 \n",
      "0.50:  0.88 0.94 0.81 \n",
      "0.55:  0.88 0.95 0.82 \n",
      "0.60:  0.89 0.96 0.82 \n",
      "0.65:  0.89 0.96 0.82 \n",
      "0.70:  0.89 0.97 0.82 \n",
      "0.75:  0.89 0.98 0.81 \n",
      "0.80:  0.88 0.99 0.80 \n",
      "0.85:  0.88 0.99 0.82 \n",
      "0.90:  0.87 0.99 0.83 \n",
      "0.95:  0.84 1.00 0.81 \n"
     ]
    }
   ],
   "source": [
    "print('Random Forest')\n",
    "print('Reject   E    A    R ')\n",
    "i = 0.00\n",
    "for e,a,r in zip(rf_ARC_eU_mean,rf_ARC_aU_mean,rf_ARC_random_mean):\n",
    "    print(f'{i:.2f}:  {e:.2f} {a:.2f} {r:.2f} ')\n",
    "    i +=step_size\n",
    "\n",
    "print()\n",
    "print('CatBoost')\n",
    "print('Reject   E    A    R ')\n",
    "i = 0.00\n",
    "for e,a,r in zip(cb_ARC_eU_mean,cb_ARC_aU_mean,cb_ARC_random_mean):\n",
    "    print(f'{i:.2f}:  {e:.2f} {a:.2f} {r:.2f} ')\n",
    "    i +=step_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = np.arange(0.0, 1.0, step_size)\n",
    "directory = 'results/'\n",
    "plot_name = 'CatBoost - Epistemische Unsicherheit Inverted'\n",
    "\n",
    "plt.plot(steps , cb_ARC_eU_mean, label=\"CatBoost\") #, marker='o', linestyle='--' , color='r'\n",
    "plt.plot(steps , cb_ARC_random_mean, label=\"Random\")\n",
    "plt.xlabel('Rejection %')\n",
    "plt.ylabel('Accuracy %') \n",
    "plt.title('')  #'Accuracy-Rejection curve'\n",
    "plt.legend() \n",
    "plt.savefig(f\"{directory}{plot_name}.png\")\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = np.arange(0.0, 1.0, step_size)\n",
    "directory = 'results/'\n",
    "plot_name = 'Random Forest - Epistemische Unsicherheit'\n",
    "\n",
    "plt.plot(steps , rf_ARC_eU_mean, label=\"Random Forest\") #, marker='o', linestyle='--' , color='r'\n",
    "plt.plot(steps , rf_ARC_random_mean, label=\"Random\")\n",
    "plt.xlabel('Rejection %')\n",
    "plt.ylabel('Accuracy %') \n",
    "plt.title('') #'Accuracy-Rejection curve'\n",
    "plt.legend() \n",
    "plt.savefig(f\"{directory}{plot_name}.png\")\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = np.arange(0.0, 1.0, step_size)\n",
    "directory = 'results/'\n",
    "plot_name = 'CatBoost - Epistemische Unsicherheit'\n",
    "\n",
    "plt.plot(steps , cb_ARC_eU_mean, label=\"CatBoost\") #, marker='o', linestyle='--' , color='r'\n",
    "plt.plot(steps , cb_ARC_random_mean, label=\"Random\")\n",
    "plt.xlabel('Rejection %')\n",
    "plt.ylabel('Accuracy %') \n",
    "plt.title('')  #'Accuracy-Rejection curve'\n",
    "plt.legend() \n",
    "plt.savefig(f\"{directory}{plot_name}.png\")\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = np.arange(0.0, 1.0, step_size)\n",
    "directory = 'results/'\n",
    "plot_name = 'Random Forest - Aleatorische Unsicherheit'\n",
    "\n",
    "plt.plot(steps , rf_ARC_aU_mean, label=\"Random Forest\") #, marker='o', linestyle='--' , color='r'\n",
    "plt.plot(steps , rf_ARC_random_mean, label=\"Random\")\n",
    "plt.xlabel('Rejection %')\n",
    "plt.ylabel('Accuracy %') \n",
    "plt.title('')   #'Accuracy-Rejection curve'\n",
    "plt.legend() \n",
    "plt.savefig(f\"{directory}{plot_name}.png\")\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = np.arange(0.0, 1.0, step_size)\n",
    "directory = 'results/'\n",
    "plot_name = 'CatBoost - Aleatorische Unsicherheit'\n",
    "\n",
    "plt.plot(steps , cb_ARC_aU_mean, label=\"CatBoost\") #, marker='o', linestyle='--' , color='r'\n",
    "plt.plot(steps , cb_ARC_random_mean, label=\"Random\")\n",
    "plt.xlabel('Rejection %')\n",
    "plt.ylabel('Accuracy %') \n",
    "plt.yticks(np.arange(0.8, 1, 0.02))\n",
    "plt.title('')   #'Accuracy-Rejection curve'\n",
    "plt.legend() \n",
    "plt.savefig(f\"{directory}{plot_name}.png\")\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = np.arange(0.0, 1.0, step_size)\n",
    "directory = 'results/'\n",
    "plot_name = 'Aleatorische Unsicherheit - CatBoost vs Random Forest'\n",
    "\n",
    "plt.plot(steps , rf_ARC_aU_mean, label=\"Random Forest\") #, marker='o', linestyle='--' , color='r'\n",
    "plt.plot(steps , cb_ARC_aU_mean, label=\"CatBoost\")\n",
    "plt.xlabel('Rejection %')\n",
    "plt.ylabel('Accuracy %') \n",
    "plt.title('')   #'Accuracy-Rejection curve'\n",
    "plt.legend() \n",
    "plt.savefig(f\"{directory}{plot_name}.png\")\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = np.arange(0.0, 1.0, step_size)\n",
    "directory = 'results/'\n",
    "plot_name = 'Epistemische Unsicherheit - CatBoost vs Random Forest'\n",
    "\n",
    "plt.plot(steps , rf_ARC_eU_mean, label=\"Random Forest\") #, marker='o', linestyle='--' , color='r'\n",
    "plt.plot(steps , cb_ARC_eU_mean, label=\"CatBoost\")\n",
    "plt.xlabel('Rejection %')\n",
    "plt.ylabel('Accuracy %') \n",
    "plt.title('')   #'Accuracy-Rejection curve'\n",
    "plt.legend() \n",
    "plt.savefig(f\"{directory}{plot_name}.png\")\n",
    "plt.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
